<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PAVLM: Advancing Point Cloud based Affordance Understanding Via Vision-Language Model">
  <meta name="keywords" content="Robotic Manipulation, Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PAVLM</title>

  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>   -->
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>

  <script>

    function updateInteractive() {
      var task = document.getElementById("interative-menu").value;

      console.log("interactive", task)

      // if task does not contain the string "towel"
      if (task.indexOf("towel") == -1) {
        var video = document.getElementById("interactive-video");
        video.src = "media/videos/" + 
                    task + 
                    ".mp4"
        video.play();

        var html = document.getElementById("interactive-html-1");
        html.src = "media/interactive/" + 
                    task + 
                    ".html"

        // hide the second iframe container
        var iframeContainer2 = document.getElementById("second-iframe-container");
        iframeContainer2.style.display = "none";
      } else {
        var video = document.getElementById("interactive-video");
        video.src = "media/videos/hang-towel.mp4"
        video.play();

        var html1 = document.getElementById("interactive-html-1");
        html1.src = "media/interactive/hang-towel-1.html"

        // show and set the source for the second iframe
        var html2 = document.getElementById("interactive-html-2");
        html2.src = "media/interactive/hang-towel-2.html"

        // show the second iframe container
        var iframeContainer2 = document.getElementById("second-iframe-container");
        iframeContainer2.style.display = "block";
      }
    }



  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    /* 为 Methodology 标题添加上下空白 */
    h2.title.is-3.has-text-centered {
      margin-top: 2rem;
      margin-bottom: 1.5rem;
    }

    /* 为内容段落添加下方空白 */
    .content p {
      margin-bottom: 2rem;
    }

    /* 为每个 figure 添加上下空白 */
    figure.image {
      margin-top: 3rem;
      margin-bottom: 3rem;
    }

    /* 为 figcaption 添加上方空白 */
    figure.image figcaption {
      margin-top: 1rem;
    }

    /* 默认样式 */
    .key-features .column {
      flex: 0 0 40%;
      max-width: 40%;
      padding: 0.75rem;
    }

    /* 当屏幕宽度小于 768px 时，调整 key-features 的子元素宽度 */
    @media (max-width: 768px) {
      .key-features .column {
        flex: 0 0 100%;
        max-width: 100%;
      }
    }

    .table-caption {
      caption-side: top;
      text-align: left;
      padding-bottom: 0.5em;
      font-weight: bold;
    }
  </style>
</head>
<body onload="updateInteractive();">

<section class="hero">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">PAVLM: Advancing Point Cloud based Affordance Understanding Via Vision-Language Model</h1>
          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://www.iros25.org/">IROS 2025 (Accept)</a></h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <!-- <a target="_blank" href="https://github.com/ShangQingLiu">Shang-Ching Liu</a> -->Shang-Ching Liu<sup>1</sup>,
            </span>
            <span class="author-block">
              <!-- <a target="_blank" href="">Van Nhiem Tran</a> -->Van Nhiem Tran<sup>2</sup>,
            </span>
            <span class="author-block">
              <!-- <a target="_blank" href="">Wenkai Chen</a> -->Wenkai Chen<sup>1*</sup>,
            </span>
            <span class="author-block">
              <!-- <a target="_blank" href="https://taiwanfifi.github.io/">Wei-Lun Cheng</a> -->Wei-Lun Cheng<sup>3</sup>,
            </span>
            <span class="author-block">
              <!-- <a target="_blank" href="">Yen-Lin Huang</a> -->Yen-Lin Huang<sup>4</sup>,
            </span>
            <span class="author-block">
              <!-- <a target="_blank" href="">I-Bin Liao</a> -->I-Bin Liao<sup>2</sup>,
            </span>
            <span class="author-block">
              <!-- <a target="_blank" href="">Yung-Hui Li</a> -->Yung-Hui Li<sup>2</sup>,
            </span>
            <span class="author-block">
              <!-- <a target="_blank" href="">Jianwei Zhang</a> -->Jianwei Zhang<sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Technical Aspects of Multimodal Systems (TAMS), Department of Informatics, Universität Hamburg,</span>
            <span class="author-block"><sup>2</sup>Hon Hai Research Institute (HHRI),</span>
            <span class="author-block"><sup>3</sup>Department of Electrical Engineering, National Taiwan University,</span>
            <span class="author-block"><sup>4</sup>Department of Computer Science and Technology, National Tsinghua University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a target="_blank" href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->

            <!-- Arxiv Link. -->
            <span class="link-block">
              <a target="_blank" href="https://arxiv.org/abs/2410.11564"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file"></i>
                </span>
                <span>ArXiv</span>
              </a>
            </span>

            <!-- Video Link. -->
            <!-- <span class="link-block">
              <a target="_blank" href="https://youtu.be/Yvn4eR05A3M"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span> -->

            <!-- Code Link. -->
            <!-- <span class="link-block">
              <a target="_blank" href="https://github.com/"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- 
<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
          <video id="teaser" autoplay muted loop height="60%" width="60%">
            <source src="media/videos/teaser.mp4"
                    type="video/mp4">
          </video>
          </br>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
        <span class="dperact">VoxPoser</span> extracts <b>affordances</b> and <b>constraints</b> from large language models and vision-language models<br>to compose 3D value maps, which are used by motion planners to <b>zero-shot synthesize</b> trajectories for everyday manipulation tasks.
        </h2>
      </div>
    </div>
  </div>
</section> -->


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <img src="static/full_shape_gif/full_0.gif" alt="GIF 1" height="100%">
        </div>
        <div class="item">
          <img src="static/full_shape_gif/full_1.gif" alt="GIF 2" height="100%">
        </div>
        <div class="item">
          <img src="static/full_shape_gif/full_2.gif" alt="GIF 3" height="100%">
        </div>
        <div class="item">
          <img src="static/partial_shape_gif/partial_0.gif" alt="GIF 4" height="100%">
        </div>
        <div class="item">
          <img src="static/partial_shape_gif/partial_1.gif" alt="GIF 5" height="100%">
        </div>
        <!-- <div class="item">
          <img src="static/partial_shape_gif/partial_2.gif" alt="GIF 6" height="100%">
        </div> -->
      </div>
    </div>
  </div>
</section>


<!-- 


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/take-out-the-toaster-and-put-it-on-the-wooden-plate.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/close-top-drawer-dist.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/sort-trash-to-tray-dist.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
           <video poster="" autoplay muted loop height="100%">
             <source src="media/videos/open-bottle.mp4"
                     type="video/mp4">
           </video>
         </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/turn-on-the-lamp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/sweep-the-trash-into-the-dustpan.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/set-up-the-utensils-for-my-pasta.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/unplug-charger.mp4"
                    type="video/mp4">
          </video>
        </div>
       <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/get-napkin.mp4"
                    type="video/mp4">
          </video>
        </div>
       <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/hang-towel.mp4"
                    type="video/mp4">
          </video>
        </div>
       <div class="item">
          <video poster="" autoplay muted loop height="100%">
            <source src="media/videos/measure-apple.mp4"
                    type="video/mp4">
          </video>
        </div>
        </div>
      </div>
    </div>
  </div>
</section> -->

<!-- <h2 class="subtitle has-text-centered">
</br>
  VoxPoser can <b>zero-shot synthesize</b> trajectories for real-world manipulation tasks with an <b>open-set</b> of free-form language instructions and an <b>open-set</b> of objects.
</h2> -->





<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Affordance understanding, the task of identifying actionable regions on 3D objects, is critical for enabling robotic systems to interact with the physical world. Although Visual Language Models (VLMs) have excelled in high-level reasoning and long-horizon planning for robotic manipulation, they still fall short in grasping the nuanced physical properties required for effective human-robot interaction. In this paper, we introduce PAVLM (Point cloud Affordance Vision-Language Model), a novel framework that leverages the rich multimodal knowledge embedded in pre-trained language models to enhance 3D affordance understanding of point cloud. PAVLM integrates a geometric-guided propagation module with hidden embeddings from large language models (LLMs) to enrich visual semantics. On the language side, we prompt LLaMa-3.1 models to generate refined context-aware text, augmenting the instructional input with deeper semantic cues. Experimental results on the 3D-AffordanceNet benchmark demonstrate that PAVLM outperforms baseline methods for both full and partial point clouds, particularly excelling in its generalization to novel open-world affordance tasks of 3D objects.
          </p>
        </div>
      </div>
    </div>


    
    <!-- Key Features and Methodology -->

    <div class="columns is-centered has-text-centered key-features">
      <div class="column is-two-fifths">
        <h2 class="title is-3">Key Features</h2>
        <div class="content has-text-justified">
          <ul>
            <li>Integration of VLMs and LLMs</li>
            <li>Geometric-guided propagation module</li>
            <li>Effective on full and partial point clouds</li>
            <li>Superior open-world performance</li>
          </ul>
        </div>
      </div>
    </div>


    <h2 class="title is-3 has-text-centered">Methodology</h2>
    <div class="content">
      <p>PAVLM combines the strengths of visual and language models to enhance 3D affordance understanding. Specifically, our geometric-guided propagation module enriches visual semantics, while LLaMa-3.1 generates refined, context-aware text instructions to better guide the robot's actions. In the following sections, we will delve into the architecture of the key components, starting with the Geometric-guided Point Encoder and Decoder. The diagram visually illustrates the overall architecture of PAVLM, highlighting the interaction between the visual and language models.</p>
    </div>
    <figure class="image">
      <img src="media/figures/fig2.png" alt="Visualization of affordance prediction results on multiple unseen objects and categories" >
      <figcaption>Our PAVLM pipeline is designed to directly encode point cloud data. We first devise a geometric-guided propagation module to extract point features. Meanwhile, Llama-3.1 is tasked with generating richer and more detailed textual instructions. Both visual and textual embeddings are aligned through the 3D Image-Bind approach, with the combined features fed into the multi-modal LLM (Llama-2) for mask label generation. Finally, the per-point feature embeddings are multiplied by the  &lt;mask label&gt; token and input into a 3D affordance decoder to generate the final affordance map. The following figure provides a detailed view of the PAVLM pipeline, illustrating each stage, from point cloud encoding to affordance map generation.</figcaption>
    </figure>
    <figure class="image">
      <img src="media/figures/fig3.png" alt="Visualization of affordance prediction results on multiple unseen objects and categories" style="width: 75%;; height: auto;">
      <figcaption>The diagram illustrates the architecture of the proposed Geometric-guided Point Encoder and Decoder within the PAVLM pipeline. It highlights the two core components: the geometric information extraction module and the feature propagation module. As shown in the figure, the point cloud data is divided into patches and processed through a series of transformer blocks to extract refined geometric features, while the decoder handles feature propagation and ensures that point-wise affordance information is accurately distributed across the point cloud.</figcaption>
    </figure>
    <figure class="image">
      <img src="media/figures/fig4.png" alt="Visualization of affordance prediction results on multiple unseen objects and categories" style="width: 70%; height: auto;">
      <figcaption>We use an augmentation prompt template to enhance the diversity of generated question-answer pairs, improving the model's contextual understanding. This strategy is particularly important for ensuring flexibility in robotic interactions. As demonstrated in the accompanying diagram, the system generates multiple versions of question-answer pairs based on a seed input.</figcaption>
    </figure>


    <!-- Experimental Results -->
    <h2 class="title is-3 has-text-centered">Experimental Results</h2>
    
    <div class="content">
      <p>We conducted extensive experiments to evaluate PAVLM's performance. Our ablation studies examined the effects of different text prompts and vision encoders, while comparisons with state-of-the-art methods demonstrated PAVLM's superiority in both seen and unseen object categories.</p>
    </div>

    <div class="columns is-multiline">
      <div class="column is-half">
        <h3 class="title is-5">Ablation Study: Text Prompts</h3>
        <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
          <caption class="table-caption">Table 1: Ablation study of different text prompts.</caption>
          <thead>
            <tr>
              <th>Point cloud</th>
              <th>Text Prompt</th>
              <th>mAP↑</th>
              <th>AUC↑</th>
              <th>aIOU↑</th>
              <th>MSE↓</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Full-shape (Aug.)</td>
              <td>Ours</td>
              <td>45.7</td>
              <td>86.9</td>
              <td>16.8</td>
              <td>0.45</td>
            </tr>
            <tr>
              <td>Full-shape</td>
              <td>Ours</td>
              <td><strong>46.0</strong></td>
              <td>87.1</td>
              <td><strong>17.1</strong></td>
              <td><strong>0.44</strong></td>
            </tr>
            <tr>
              <td>Full-shape</td>
              <td>Object, Action</td>
              <td>44.5</td>
              <td>85.6</td>
              <td>16.0</td>
              <td>0.67</td>
            </tr>
            <tr>
              <td>Full-shape</td>
              <td>Action</td>
              <td>45.4</td>
              <td><strong>87.2</strong></td>
              <td>16.7</td>
              <td>0.45</td>
            </tr>
            <tr>
              <td>Full-shape</td>
              <td>Hi</td>
              <td>11.4</td>
              <td>51.9</td>
              <td>0.4</td>
              <td>0.90</td>
            </tr>
          </tbody>
        </table>
      </div>

      <div class="column is-half">
        <h3 class="title is-5">Ablation Study: Vision Encoders</h3>
        <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
          <caption class="table-caption">Table 2: Ablation study of different vision encoders.</caption>
          <thead>
            <tr>
              <th>Point cloud</th>
              <th>Vision encoder</th>
              <th>mAP↑</th>
              <th>AUC↑</th>
              <th>aIOU↑</th>
              <th>MSE↓</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Full-shape</td>
              <td>PointNet++</td>
              <td>47.1</td>
              <td>86.2</td>
              <td><strong>18.1</strong></td>
              <td><strong>0.36</strong></td>
            </tr>
            <tr>
              <td>Full-shape</td>
              <td>DGCNN</td>
              <td>31.0</td>
              <td>72.9</td>
              <td>1.0</td>
              <td>0.68</td>
            </tr>
            <tr>
              <td>Full-shape</td>
              <td>Ours</td>
              <td><strong>48.5</strong></td>
              <td><strong>86.8</strong></td>
              <td>17.7</td>
              <td><strong>0.36</strong></td>
            </tr>
            <tr>
              <td>Partial-shape</td>
              <td>PointNet++</td>
              <td>25.3</td>
              <td>69.8</td>
              <td>3.4</td>
              <td>0.73</td>
            </tr>
            <tr>
              <td>Partial-shape</td>
              <td>DGCNN</td>
              <td>26.8</td>
              <td>69.7</td>
              <td>0.7</td>
              <td>0.67</td>
            </tr>
            <tr>
              <td>Partial-shape</td>
              <td>Ours</td>
              <td><strong>40.2</strong></td>
              <td><strong>82.2</strong></td>
              <td><strong>11.7</strong></td>
              <td><strong>0.39</strong></td>
            </tr>
          </tbody>
        </table>
      </div>

      <div class="column is-half">
        <h3 class="title is-5">Comparison: Seen Categories</h3>
        <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
          <caption class="table-caption">Table 3: Comparison results with baselines on the seen split of dataset</caption>
          <thead>
            <tr>
              <th>Point cloud</th>
              <th>Method</th>
              <th>mAP↑</th>
              <th>AUC↑</th>
              <th>aIOU↑</th>
              <th>MSE↓</th>
            </tr>
          </thead>
          <tbody>
            <!-- Full-shape rows -->
            <tr>
              <td>Full-shape</td>
              <td>PointCLIP</td>
              <td>7.6</td>
              <td>49.9</td>
              <td>0.9</td>
              <td>0.80</td>
            </tr>
            <tr>
              <td>Full-shape</td>
              <td>PointCLIP V2</td>
              <td>7.6</td>
              <td>50.0</td>
              <td>0.8</td>
              <td>0.70</td>
            </tr>
            <tr>
              <td>Full-shape</td>
              <td>ULIP</td>
              <td>7.6</td>
              <td>50.0</td>
              <td>0.4</td>
              <td>0.60</td>
            </tr>
            <tr>
              <td>Full-shape</td>
              <td>ULIP2</td>
              <td>7.6</td>
              <td>50.1</td>
              <td>1.2</td>
              <td>0.63</td>
            </tr>
            <tr>
              <td>Full-shape</td>
              <td>Ours (frozen)</td>
              <td>46.3</td>
              <td><strong>86.9</strong></td>
              <td>17.1</td>
              <td>0.44</td>
            </tr>
            <tr>
              <td>Full-shape</td>
              <td>Ours</td>
              <td><strong>48.5</strong></td>
              <td>86.8</td>
              <td><strong>17.7</strong></td>
              <td><strong>0.36</strong></td>
            </tr>
      
            <!-- Partial-shape rows -->
            <tr>
              <td>Partial-shape</td>
              <td>PointCLIP</td>
              <td>9.1</td>
              <td>50.1</td>
              <td>1.1</td>
              <td>0.78</td>
            </tr>
            <tr>
              <td>Partial-shape</td>
              <td>PointCLIP V2</td>
              <td>9.2</td>
              <td>49.9</td>
              <td>1.4</td>
              <td>0.66</td>
            </tr>
            <tr>
              <td>Partial-shape</td>
              <td>ULIP</td>
              <td>9.2</td>
              <td>50.0</td>
              <td>1.0</td>
              <td>0.58</td>
            </tr>
            <tr>
              <td>Partial-shape</td>
              <td>ULIP2</td>
              <td>9.2</td>
              <td>50.1</td>
              <td>1.1</td>
              <td>0.55</td>
            </tr>
            <tr>
              <td>Partial-shape</td>
              <td>Ours (frozen)</td>
              <td>37.2</td>
              <td>64.2</td>
              <td>11.4</td>
              <td><strong>0.36</strong></td>
            </tr>
            <tr>
              <td>Partial-shape</td>
              <td>Ours</td>
              <td><strong>40.2</strong></td>
              <td><strong>82.2</strong></td>
              <td><strong>11.7</strong></td>
              <td>0.39</td>
            </tr>
          </tbody>
        </table>
      </div>
      
      <div class="column is-half">
        <h3 class="title is-5">Comparison: Unseen Categories</h3>
        <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
          <caption class="table-caption">Table 4: Comparison results with baselines on the unseen split of dataset</caption>
          <thead>
            <tr>
              <th>Point cloud</th>
              <th>Method</th>
              <th>mAP↑</th>
              <th>AUC↑</th>
              <th>aIOU↑</th>
              <th>MSE↓</th>
            </tr>
          </thead>
          <tbody>
            <!-- Full-shape rows -->
            <tr>
              <td>Full-shape</td>
              <td>PointCLIP</td>
              <td>3.7</td>
              <td>49.9</td>
              <td>0.5</td>
              <td>1.58</td>
            </tr>
            <tr>
              <td>Full-shape</td>
              <td>PointCLIP V2</td>
              <td>3.7</td>
              <td>49.3</td>
              <td>0.4</td>
              <td>0.91</td>
            </tr>
            <tr>
              <td>Full-shape</td>
              <td>ULIP</td>
              <td>3.8</td>
              <td>50.2</td>
              <td>0.17</td>
              <td>0.53</td>
            </tr>
            <tr>
              <td>Full-shape</td>
              <td>ULIP2</td>
              <td>3.8</td>
              <td>49.9</td>
              <td>0.22</td>
              <td>0.70</td>
            </tr>
            <tr>
              <td>Full-shape</td>
              <td>Ours (frozen)</td>
              <td>11.8</td>
              <td><strong>60.3</strong></td>
              <td>1.5</td>
              <td>0.78</td>
            </tr>
            <tr>
              <td>Full-shape</td>
              <td>Ours</td>
              <td><strong>12.1</strong></td>
              <td>53.9</td>
              <td><strong>2.6</strong></td>
              <td><strong>0.51</strong></td>
            </tr>
      
            <!-- Partial-shape rows -->
            <tr>
              <td>Partial-shape</td>
              <td>PointCLIP</td>
              <td>4.7</td>
              <td>50.2</td>
              <td>0.6</td>
              <td>1.14</td>
            </tr>
            <tr>
              <td>Partial-shape</td>
              <td>PointCLIP V2</td>
              <td>4.7</td>
              <td>50.3</td>
              <td>0.38</td>
              <td>0.93</td>
            </tr>
            <tr>
              <td>Partial-shape</td>
              <td>ULIP</td>
              <td>4.8</td>
              <td>50.6</td>
              <td>0.23</td>
              <td>0.66</td>
            </tr>
            <tr>
              <td>Partial-shape</td>
              <td>ULIP2</td>
              <td>4.8</td>
              <td>50.0</td>
              <td>0.37</td>
              <td>0.61</td>
            </tr>
            <tr>
              <td>Partial-shape</td>
              <td>Ours (frozen)</td>
              <td>13.8</td>
              <td>60.3</td>
              <td>2.0</td>
              <td><strong>0.60</strong></td>
            </tr>
            <tr>
              <td>Partial-shape</td>
              <td>Ours</td>
              <td><strong>22.3</strong></td>
              <td><strong>63.2</strong></td>
              <td><strong>2.1</strong></td>
              <td>0.82</td>
            </tr>
          </tbody>
        </table>
      </div>

    <div class="content">
      <p>Our results show that PAVLM consistently outperforms baseline methods across various metrics. The ablation studies reveal that comprehensive question prompts and our proposed geometric-guided point encoder contribute significantly to the model's performance. Furthermore, PAVLM demonstrates strong generalization capabilities, particularly in handling unseen object categories.</p>
    </div>
  

    <div class="container">
      <h2 class="title is-3 has-text-centered">Visualization of Results</h2>
      <figure class="image">
        <img src="media/figures/affordance_prediction_visualization.png" alt="Visualization of affordance prediction results on multiple unseen objects and categories" style="width: 90%; height: auto;">
        <figcaption>Visualization of affordance prediction results on multiple unseen objects and categories, showing that our model predicts accurate affordances guided by different instructions from the original partial point cloud.</figcaption>
      </figure>
    </div>

  </div>
</section>

<!-- 
<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{ShangQingLiu2025PAVLM,
      title={PAVLM: Advancing Point Cloud based Affordance Understanding Via Vision-Language Model},
      author={Shang-Ching Liu, Van Nhiem Tran, Wenkai Chen, Wei-Lun Cheng, Yen-Lin Huang, I-Bin Liao, Yung-Hui Li, Jianwei Zhang},
      journal={},
      year={2025}
    }</code></pre>
  </div>
</section>
 -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a href="https://peract.github.io/">PerAct</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>



</body>
</html>
